{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBRoIE2pKtb9"
      },
      "source": [
        "<img src=\"./image/labai.png\" width=\"200px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQbkf3OXKtb_"
      },
      "source": [
        "# Text Generation with GRU\n",
        "\n",
        "In this exercise your goal is to build text generation model with GRU model by complete all piece of code below, you can add or change code as we can\n",
        "\n",
        "\n",
        "**Objective**:  \n",
        "In this exercise, your goal is to build a text generation model using a Gated Recurrent Unit (GRU). You will complete all the provided code segments and are encouraged to add or modify code to improve the model. The key steps involve:\n",
        "\n",
        "1. Preprocessing the text data.\n",
        "2. Implementing the GRU-based neural network.\n",
        "3. Training the model on the provided dataset.\n",
        "4. Generating new text based on a seed sequence.\n",
        "\n",
        "**Instructions**:\n",
        "- Follow the code structure provided and complete the missing sections.\n",
        "- Experiment with different hyperparameters to improve performance.\n",
        "- You are free to adjust the code as needed to enhance results.\n",
        "\n",
        "**Please use Google colab for free GPU**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.2.0 torchtext==0.17.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-miTPPxLLl_",
        "outputId": "b716527b-805e-4065-b01a-54f280d47f77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.2.0\n",
            "  Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting torchtext==0.17.0\n",
            "  Downloading torchtext-0.17.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.0)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.0)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.2.0 (from torch==2.2.0)\n",
            "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.17.0) (4.66.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.17.0) (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.17.0) (1.26.4)\n",
            "Collecting torchdata==0.7.1 (from torchtext==0.17.0)\n",
            "  Downloading torchdata-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0) (12.6.77)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.1->torchtext==0.17.0) (2.2.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.17.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.17.0) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.17.0) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.0) (1.3.0)\n",
            "Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchtext-0.17.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchdata-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchdata, torchtext\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.3.3\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.3.3:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.3.3\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.5.1.17\n",
            "    Uninstalling nvidia-cudnn-cu12-9.5.1.17:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.0+cu121\n",
            "    Uninstalling torch-2.5.0+cu121:\n",
            "      Successfully uninstalled torch-2.5.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.0+cu121 requires torch==2.5.0, but you have torch 2.2.0 which is incompatible.\n",
            "torchvision 0.20.0+cu121 requires torch==2.5.0, but you have torch 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 torch-2.2.0 torchdata-0.7.1 torchtext-0.17.0 triton-2.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIL1aQqrN6QK",
        "outputId": "b5ec60e2-41fd-4c44-81eb-48ca73043fc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lightning\n",
            "  Downloading lightning-2.4.0-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.10.0)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning)\n",
            "  Downloading lightning_utilities-0.11.8-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (24.1)\n",
            "Requirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.2.0)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning)\n",
            "  Downloading torchmetrics-1.5.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.12.2)\n",
            "Collecting pytorch-lightning (from lightning)\n",
            "  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.10.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (75.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<4.0,>=2.1.0->lightning) (12.6.77)\n",
            "Requirement already satisfied: numpy<2.0,>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics<3.0,>=0.7.0->lightning) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<4.0,>=2.1.0->lightning) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.10)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (0.2.0)\n",
            "Downloading lightning-2.4.0-py3-none-any.whl (810 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m811.0/811.0 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.8-py3-none-any.whl (26 kB)\n",
            "Downloading torchmetrics-1.5.1-py3-none-any.whl (890 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.6/890.6 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, pytorch-lightning, lightning\n",
            "Successfully installed lightning-2.4.0 lightning-utilities-0.11.8 pytorch-lightning-2.4.0 torchmetrics-1.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ndrKrakKtcA",
        "outputId": "9de700c8-a5ed-409c-fe07-cd679209d6bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "# import sommes packages\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "import re\n",
        "import torch\n",
        "import torchtext\n",
        "import torch.nn as nn\n",
        "from pathlib import Path\n",
        "from typing import List,Dict\n",
        "\n",
        "import lightning as L\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import unicodedata\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.vocab  import build_vocab_from_iterator\n",
        "from lightning.pytorch.loggers import TensorBoardLogger\n",
        "\n",
        "# Attempt GPU; if not, stay on CPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exLafw7dKtcB"
      },
      "source": [
        "### I- Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70cViY8bKtcB"
      },
      "outputs": [],
      "source": [
        "# load dataset\n",
        "text = Path('./data/tiny-shakespeare.txt').read_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5I5zIltKtcC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2157f583-ac38-4432-ae99-26fa264c2d0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of characters in text file: 1,115,394\n"
          ]
        }
      ],
      "source": [
        "# print total number of characters:\n",
        "print(f'Number of characters in text file: {len(text):,}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwwOe-tJ-xcE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15e755a2-922a-4474-80f5-4c1a9746d17e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor\n"
          ]
        }
      ],
      "source": [
        "print(text[0:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Umy6uc_3KtcC"
      },
      "source": [
        "## II - Word-Based Text Generation\n",
        "\n",
        "The first model you'll build for **text generation** will use Word-based tokens. Each token will be a single word from the text and the model will learn to predict the next word (a token).\n",
        "\n",
        "To generate text, the model will take in a new string, word-by-word, and then generate a new likely word based on the past input. Then the model will take into account that new word and generate the following word and so on and so on until the model has produced a set number of word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxoMH5zRKtcD"
      },
      "source": [
        "### II.1  Tokenization :\n",
        "Create a tokenizer that will create tokens by character"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "up-dLDe1KtcE"
      },
      "outputs": [],
      "source": [
        "\n",
        "class  WordTokenizer(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab: torchtext.vocab.Vocab|Dict[str,int])-> None:\n",
        "        super().__init__()\n",
        "\n",
        "        if isinstance(vocab, torchtext.vocab.Vocab):\n",
        "            self.token2id=vocab.get_stoi()\n",
        "            self.id2token={id:ch for ch,id in vocab.get_stoi().items()}\n",
        "            self.vocab_size=len(self.token2id)\n",
        "\n",
        "        elif isinstance(vocab, dict):\n",
        "            self.token2id=vocab\n",
        "            self.id2token={id:ch for ch,id in vocab.items()}\n",
        "            self.vocab_size=len(self.token2id)\n",
        "\n",
        "        else:\n",
        "            raise TypeError(\"Please loads a vocabulary file into a dictionary \\\n",
        "                            Dict[str,int] or torchtext.vocab.Vocab\")\n",
        "\n",
        "    def encode(self, text:List[str]|str):\n",
        "        if isinstance(text, str):\n",
        "            text_list=self.tokenize(text)\n",
        "\n",
        "        tokenid=[]\n",
        "        for token in text_list:\n",
        "            tokenid.append(self.token2id[token])\n",
        "        return  torch.tensor(tokenid,  dtype=torch.long)\n",
        "\n",
        "\n",
        "    def decode(self, idx:torch.tensor):\n",
        "        #idx: torch.Tensor containing integers\n",
        "        token=[]\n",
        "        for id in idx.tolist():\n",
        "            token.append(self.id2token[id])\n",
        "        return ' '.join(token)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenize(text: str) -> List[str]:\n",
        "\n",
        "        # Normalize incoming text; can be multiple actions\n",
        "        text= text.lower().strip() ## Your code Here ##\n",
        "\n",
        "        # split text into tokens\n",
        "        tokens= text.split() ## Your code Here ##\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    @staticmethod\n",
        "    def _tokenizer_corpus(corpus:List[str]):\n",
        "        for text in corpus:\n",
        "            yield WordTokenizer.tokenize(text)\n",
        "\n",
        "    @staticmethod\n",
        "    def train_from_text(text: str) -> List[str]:\n",
        "        \"\"\"build vocab from one text corpus\"\"\"\n",
        "        vocab=build_vocab_from_iterator(WordTokenizer._tokenizer_corpus(WordTokenizer.tokenize(text)),\n",
        "                                        specials=[\"<unk>\"]\n",
        "                                       )\n",
        "        vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "        return WordTokenizer(vocab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICxWh_-OKtcE"
      },
      "outputs": [],
      "source": [
        "# create tokenizer from text\n",
        "tokenizer = WordTokenizer.train_from_text(text)## Your code Here ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zm_DkYJyKtcF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c03480bb-0a17-43e0-b954-21dc291339eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['first', 'citizen:', 'before', 'we', 'proceed', 'any', 'further,', 'hear', 'me', 'speak.', 'all:', 'speak,', 'speak.', 'first', 'citizen:', 'you', 'are', 'all', 'resolved', 'rather', 'to', 'die', 'than', 'to', 'famish?', 'all:', 'resolved.', 'resolved.', 'first', 'citizen:', 'first,', 'you', 'know', 'caius', 'marcius', 'is', 'chief', 'enemy', 'to', 'the', 'people.', 'all:', 'we', \"know't,\", 'we', \"know't.\", 'first', 'citizen:', 'let', 'us']\n"
          ]
        }
      ],
      "source": [
        "# show example of word-based tokens\n",
        "print(tokenizer.tokenize(text[0:300]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BymcmFv8KtcF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9916f34b-636d-490b-a1db-d717518bdbb2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 533,    3,    1,  592, 4449, 4180])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# tokenization\n",
        "encode_text=tokenizer.encode(\"Welcome to the deep learning course.\")\n",
        "encode_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJtW2HtaKtcG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5c1f39d8-02f2-4863-ae78-9a529d3bcdea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'welcome to the deep learning course.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "decode_text=tokenizer.decode(encode_text)\n",
        "decode_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJY-PT6dKtcG"
      },
      "source": [
        "### III - Prepare dataset for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d98JqDBCKtcG"
      },
      "outputs": [],
      "source": [
        "class shakespeareDataset(Dataset):\n",
        "    def __init__(self, encode_text, max_seq_length: int):\n",
        "        self.encode_text     = encode_text\n",
        "        self.max_seq_length  = max_seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encode_text)-self.max_seq_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        assert idx < len(self.encode_text)-self.max_seq_length\n",
        "\n",
        "        x_train= self.encode_text[idx:idx+self.max_seq_length]\n",
        "\n",
        "        # Target is shifted by one character/token\n",
        "        y_target= self.encode_text[idx+1:idx+1+self.max_seq_length]\n",
        "\n",
        "        return x_train, y_target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gd2fhqjjKtcG"
      },
      "outputs": [],
      "source": [
        "dataset=shakespeareDataset(encode_text=tokenizer.encode(text),max_seq_length=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__ru5DbcKtcG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4921dd0-394c-48dd-dd92-7d601fe7676b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([   82,   225,   147,    31,  1650,   128,  4313,   124,    25,   561,\n",
              "           650,   547,   561,    82,   225,     8,    36,    35,  1954,   319,\n",
              "             3,   332,    52,     3, 14444,   650,  9129,  9129,    82,   225,\n",
              "           569,     8,    87,  1273,   675,    11,  2936,  1091,     3,     1,\n",
              "          1645,   650,    31, 16696,    31,  5774,    82,   225,    57,    97,\n",
              "           494,   140,     2,   306,    21,  2562,    46,    34,   170, 19178,\n",
              "           639,     7, 22816,   650,    40,    54,  4835,  8748,    57,    22,\n",
              "            16,  1716,   712,   849,   142,   225,    69,   912,    43,  7216,\n",
              "            82,   225,    31,    36,  6708,   159,  2940,     1,  4599,  1499,\n",
              "            28,  2519, 21556,    44,    48,  6135,  1445,    33,    49,    48]),\n",
              " tensor([  225,   147,    31,  1650,   128,  4313,   124,    25,   561,   650,\n",
              "           547,   561,    82,   225,     8,    36,    35,  1954,   319,     3,\n",
              "           332,    52,     3, 14444,   650,  9129,  9129,    82,   225,   569,\n",
              "             8,    87,  1273,   675,    11,  2936,  1091,     3,     1,  1645,\n",
              "           650,    31, 16696,    31,  5774,    82,   225,    57,    97,   494,\n",
              "           140,     2,   306,    21,  2562,    46,    34,   170, 19178,   639,\n",
              "             7, 22816,   650,    40,    54,  4835,  8748,    57,    22,    16,\n",
              "          1716,   712,   849,   142,   225,    69,   912,    43,  7216,    82,\n",
              "           225,    31,    36,  6708,   159,  2940,     1,  4599,  1499,    28,\n",
              "          2519, 21556,    44,    48,  6135,  1445,    33,    49,    48,   580]))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# check\n",
        "dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23h7nA2FKtcH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "2e06fb31-2dcd-44c3-e8a8-0dd7b429e67a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"first citizen: before we proceed any further, hear me speak. all: speak, speak. first citizen: you are all resolved rather to die than to famish? all: resolved. resolved. first citizen: first, you know caius marcius is chief enemy to the people. all: we know't, we know't. first citizen: let us kill him, and we'll have corn at our own price. is't a verdict? all: no more talking on't; let it be done: away, away! second citizen: one word, good citizens. first citizen: we are accounted poor citizens, the patricians good. what authority surfeits on would relieve us: if they would\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# check\n",
        "tokenizer.decode(dataset[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ot6R3tEtKtcH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "5bdd085c-10d7-4d41-e225-e8f7423ab5a1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"citizen: before we proceed any further, hear me speak. all: speak, speak. first citizen: you are all resolved rather to die than to famish? all: resolved. resolved. first citizen: first, you know caius marcius is chief enemy to the people. all: we know't, we know't. first citizen: let us kill him, and we'll have corn at our own price. is't a verdict? all: no more talking on't; let it be done: away, away! second citizen: one word, good citizens. first citizen: we are accounted poor citizens, the patricians good. what authority surfeits on would relieve us: if they would yield\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# check\n",
        "tokenizer.decode(dataset[1][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Cont1c-KtcH"
      },
      "outputs": [],
      "source": [
        "# batch dataset\n",
        "train_dataloader = DataLoader(dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB67HHjrKtcH"
      },
      "source": [
        "### Build GRU model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "np8kv6xxKtcI"
      },
      "outputs": [],
      "source": [
        "class GRUTextGen(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout):\n",
        "\n",
        "        super(GRUTextGen, self).__init__()\n",
        "\n",
        "\n",
        "        assert 0 <= dropout <=1 , \"dropout value must be between [0,1]\"\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.embedding=nn.Embedding(vocab_size,embed_size)\n",
        "\n",
        "\n",
        "        self.gru=nn.GRU(\n",
        "            input_size=embed_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout if num_layers > 1 else 0,  # Only apply dropout if more than 1 layer\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.fc=nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.tensor):\n",
        "        assert x.ndim==2, \"x tensor must be 2D dimensions with shape (B,S), B=batch, S=sequence length\"\n",
        "        x = self.embedding(x)\n",
        "        output, h = self.gru(x)\n",
        "        logits    = self.fc(output)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzLGdgljKtcI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "041a5eb7-bb2e-443f-a2ae-6fd5da9430a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GRUTextGen(\n",
              "  (embedding): Embedding(24000, 100)\n",
              "  (gru): GRU(100, 128, num_layers=2, batch_first=True, dropout=0.3)\n",
              "  (fc): Linear(in_features=128, out_features=24000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "GRU_model = GRUTextGen(\n",
        "    vocab_size= 24000 ,\n",
        "    embed_size= 100,\n",
        "    hidden_size= 128,\n",
        "    num_layers=2,\n",
        "    dropout = 0.3\n",
        ")\n",
        "\n",
        "\n",
        "GRU_model.to(device)\n",
        "GRU_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0501BsE7KtcI"
      },
      "source": [
        "## Inference mode: Define Text Generation :\n",
        "Generate text with a character-based model\n",
        "\n",
        "The `generate_text_by_word` function will use your tokenizer and LSTM model to generate new text token-by-token by taking in the input text and token sampling parameters. We can use temperature and top-k sampling to adjust the \"creativeness\" of the generated text.\n",
        "\n",
        "We also pass in the num_tokens parameter to tell the function how many tokens to generate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrnIHOygKtcI"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def generate_text_by_word(input_text:str, max_tokens:int=15,\n",
        "                          temperature:int=1, top_k:int|None=None,\n",
        "                          do_sample:bool=False,\n",
        "                        tokenizer=tokenizer):\n",
        "\n",
        "    \"\"\"Inference: Define Text Generation\"\"\"\n",
        "    idx=tokenizer.encode(input_text).unsqueeze(dim=0).to(device)\n",
        "\n",
        "    max_sequence_length=31\n",
        "\n",
        "    assert idx.ndim==2, \"input token must be 2D with sahpe (B, S) B batch,S sequence Length\"\n",
        "\n",
        "    for _ in range(max_tokens): # The maximum number of tokens that can be generated\n",
        "        # if the sequence context is growing too long we must crop it at context_size\n",
        "        idx_cond=idx if idx.size(1)<=max_sequence_length else idx[:,-max_sequence_length:]\n",
        "\n",
        "        # forward the model to get the logits for the index in the sequence\n",
        "        logits=GRU_model(idx_cond)\n",
        "\n",
        "        # pluck the logits at the final step and scale by desired temperature\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "\n",
        "        if top_k is not None:\n",
        "            values= torch.topk(logits, top_k).values\n",
        "            logits[logits < values[:,[-1]]]=-torch.inf\n",
        "\n",
        "        # apply softmax to convert logits to (normalized) probabilities\n",
        "        probs =F.softmax(logits, dim=-1)\n",
        "\n",
        "        if do_sample:\n",
        "            idx_next=torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            idx_next=torch.topk(probs, k=1, dim=-1).indices  # greedy decoding\n",
        "\n",
        "        # append sampled index to the running sequence and continue\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return tokenizer.decode(idx.squeeze())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEcW-9NEKtcJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d14b09b1-ebad-49a9-ec6f-f87b12ca997f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'to be or not to be foes hearty greatest issue; deputy discredits. proceedings powers, proceedings hate, hate, cloud, commanded somerset? wretches'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# check text generation without training model\n",
        "TEST_PHRASE = 'To be or not to be'\n",
        "generate_text_by_word(TEST_PHRASE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U5E2vNwKtcJ"
      },
      "source": [
        "## Train GRU :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJKXa87MKtcJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f688085f-fac0-4292-badb-cae651ddc6ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 7.2576527661810175\n",
            "------------------------------------------------------------------------\n",
            "to be or not to be a sebastian: as i am a month i am not a month gonzalo: to be a dukedom. gonzalo: i am not so a brother. gonzalo: i am not to be\n",
            "Epoch 2/5, Loss: 6.240594867206109\n",
            "------------------------------------------------------------------------\n",
            "to be or not to be a kind of milan, antonio: and i have no more than a match! to the rest of the king's ship; and the rest of the earth of the island. of\n",
            "Epoch 3/5, Loss: 5.729409746197162\n",
            "------------------------------------------------------------------------\n",
            "to be or not to be a hair of the sea, of the isle, and all the earth of the earth of the island. gonzalo: i have no more than this island of the sea, of\n",
            "Epoch 4/5, Loss: 5.346152390424297\n",
            "------------------------------------------------------------------------\n",
            "to be or not to be inclined and not a kind of tunis. sebastian: i am not to be desert,-- gonzalo: i have no more amazement: than i am a man. gonzalo: if you be a\n",
            "Epoch 5/5, Loss: 5.03039323827855\n",
            "------------------------------------------------------------------------\n",
            "to be or not to be desert,-- and i have a man. gonzalo: this is the wager? gonzalo: antonio: he kept, sebastian: alonso: alonso: sir, i have done to the entertainer-- sebastian: of the marriage of\n"
          ]
        }
      ],
      "source": [
        "GRU_model = GRU_model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(GRU_model.parameters(), lr=0.001)\n",
        "\n",
        "# Use more epochs if not CPU device\n",
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Set model into \"training mode\"\n",
        "    GRU_model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for X_batch, y_batch in train_dataloader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = GRU_model(X_batch)\n",
        "        loss   = criterion(output.view(-1, output.size(-1)), y_batch.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_dataloader)}')\n",
        "    print('-'*72)\n",
        "\n",
        "    gen_output = generate_text_by_word(\n",
        "        input_text=TEST_PHRASE,\n",
        "        temperature=0.8,\n",
        "        max_tokens=30,\n",
        "        top_k=None,\n",
        "        do_sample=False,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "    print(gen_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XafGfK3HKtcJ"
      },
      "source": [
        "## Generate Text\n",
        "\n",
        "Now that the model has been trained, go ahead and observe how it performs!\n",
        "\n",
        "Try adjusting the different sampling methods using the `temperature` and `topk`\n",
        "parameters on the same input string to see the differences.\n",
        "\n",
        "You might also try different phrases as well as how many tokens  to generate and observe how it does."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1zX_3JpKtcJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7088844-f176-4ee1-9666-bff0dfd1abe5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "to be or to the rest sebastian: a dollar. gonzalo: and the mariners of the curl'd nook, and the son owes. antonio: antonio:\n"
          ]
        }
      ],
      "source": [
        "output = generate_text_by_word(\n",
        "    input_text='To be or ',\n",
        "    max_tokens=20,\n",
        "    do_sample=False,\n",
        "    tokenizer=tokenizer,\n",
        "    temperature=1.0,\n",
        "    top_k=None,\n",
        ")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tpbej52uKtcK"
      },
      "source": [
        "Great Job 👏"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}